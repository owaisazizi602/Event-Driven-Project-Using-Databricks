{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fad06d08-628c-4c0f-b0f2-457e9e807f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Merge Operation and Data Consolidation\n",
    "This notebook performs the final merge operation to consolidate all processed data into target tables with SCD2 (Slowly Changing Dimension) logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007b3c0c-841e-4e33-8e27-e82db99f8ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from delta.tables import *\n",
    "\n",
    "# Source tables\n",
    "enriched_orders_table = \"`incremental_load`.default.enriched_orders\"\n",
    "customer_analytics_table = \"`incremental_load`.default.customer_analytics\"\n",
    "product_analytics_table = \"`incremental_load`.default.product_analytics\"\n",
    "\n",
    "# Target tables\n",
    "orders_target = \"`incremental_load`.default.orders_target\"\n",
    "customers_target = \"`incremental_load`.default.customers_target\"\n",
    "products_target = \"`incremental_load`.default.products_target\"\n",
    "inventory_target = \"`incremental_load`.default.inventory_target\"\n",
    "shipping_target = \"`incremental_load`.default.shipping_target\"\n",
    "analytics_summary_table = \"`incremental_load`.default.analytics_summary\"\n",
    "\n",
    "print(\"Starting final merge operation...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa0990b-f4d7-43cf-9d92-f3fc7f47be8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Read enriched data\n",
    "try:\n",
    "    df_enriched_orders = spark.read.table(enriched_orders_table)\n",
    "    df_customer_analytics = spark.read.table(customer_analytics_table)\n",
    "    df_product_analytics = spark.read.table(product_analytics_table)\n",
    "    \n",
    "    print(\"Successfully loaded enriched datasets\")\n",
    "    print(f\"Enriched orders: {df_enriched_orders.count()} records\")\n",
    "    print(f\"Customer analytics: {df_customer_analytics.count()} records\")\n",
    "    print(f\"Product analytics: {df_product_analytics.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading enriched datasets: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b07d7c-1ba8-48e8-a3d6-e81ae77b7245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge Orders Data with SCD2 Logic\n",
    "try:\n",
    "    # Prepare orders data for merge\n",
    "    df_orders_merge = df_enriched_orders.select(\n",
    "        \"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"order_amount\",\n",
    "        \"currency\", \"payment_method\", \"shipping_address\", \"order_status\",\n",
    "        \"created_timestamp\", \"processed_timestamp\", \"batch_id\", \"source_system\",\n",
    "        \"order_profit_margin\", \"estimated_clv\", \"season\", \"day_of_week\",\n",
    "        \"is_weekend\", \"time_of_day\"\n",
    "    ).withColumn(\"effective_date\", F.current_date()) \\\n",
    "     .withColumn(\"expiry_date\", F.lit(None).cast(DateType())) \\\n",
    "     .withColumn(\"is_current\", F.lit(True))\n",
    "    \n",
    "    # Check if target table exists\n",
    "    if spark.catalog.tableExists(orders_target):\n",
    "        # Perform SCD2 merge\n",
    "        target_orders = DeltaTable.forName(spark, orders_target)\n",
    "        \n",
    "        # Set expiry date for existing records that will be updated\n",
    "        target_orders.update(\n",
    "            condition=F.col(\"order_id\").isin([row.order_id for row in df_orders_merge.select(\"order_id\").distinct().collect()]),\n",
    "            set={\n",
    "                \"expiry_date\": F.current_date(),\n",
    "                \"is_current\": F.lit(False)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Insert new records\n",
    "        df_orders_merge.write.format(\"delta\").mode(\"append\").saveAsTable(orders_target)\n",
    "        \n",
    "    else:\n",
    "        # Create new table\n",
    "        df_orders_merge.write.format(\"delta\").saveAsTable(orders_target)\n",
    "    \n",
    "    print(\"Orders merge completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error merging orders data: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406a1c0f-817b-4ca0-8d03-4901af6e90b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge Customers Data with SCD2 Logic\n",
    "\n",
    "try:\n",
    "    df_customers_merge = df_customer_analytics.select(\n",
    "        \"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\",\n",
    "        \"date_of_birth\", \"registration_date\", \"address\", \"city\", \"state\",\n",
    "        \"zip_code\", \"country\", \"customer_tier\", \"last_login\", \"customer_created_timestamp\",\n",
    "        \"age\", \"age_segment\", \"days_since_registration\", \"lifecycle_stage\",\n",
    "        \"total_orders\", \"total_spent\", \"avg_order_value\", \"customer_segment\"\n",
    "    ).withColumn(\"effective_date\", F.current_date()) \\\n",
    "     .withColumn(\"expiry_date\", F.lit(None).cast(DateType())) \\\n",
    "     .withColumn(\"is_current\", F.lit(True))\n",
    "    \n",
    "    if spark.catalog.tableExists(customers_target):\n",
    "        target_customers = DeltaTable.forName(spark, customers_target)\n",
    "        target_customers.update(\n",
    "            condition=F.col(\"customer_id\").isin([row.customer_id for row in df_customers_merge.select(\"customer_id\").distinct().collect()]),\n",
    "            set={\n",
    "                \"expiry_date\": F.current_date(),\n",
    "                \"is_current\": F.lit(False)\n",
    "            }\n",
    "        )\n",
    "        df_customers_merge.write.format(\"delta\").mode(\"append\").saveAsTable(customers_target)\n",
    "    else:\n",
    "        df_customers_merge.write.format(\"delta\").saveAsTable(customers_target)\n",
    "    print(\"Customers merge completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging customers data: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7c70ef-67c2-4274-8698-c66547bdd482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge Products Data with SCD2 Logic\n",
    "try:\n",
    "    # Prepare products data for merge\n",
    "    df_products_merge = df_product_analytics.select(\n",
    "        \"product_id\", \"product_name\", \"category\", \"subcategory\", \"brand\",\n",
    "        \"price\", \"product_currency\", \"product_stock_quantity\", \"product_weight_kg\", \"dimensions_cm\",\n",
    "        \"color\", \"material\", \"description\", \"launch_date\", \"discontinued\",\n",
    "        \"product_created_timestamp\", \"product_price_segment\", \"product_stock_status\", \"days_since_launch\",\n",
    "        \"product_lifecycle_stage\", \"volume_cm3\", \"density_kg_per_cm3\", \"total_orders\",\n",
    "        \"total_revenue\", \"unique_customers\", \"performance_category\", \"product_lifecycle\"\n",
    "    ).withColumn(\"effective_date\", F.current_date()) \\\n",
    "     .withColumn(\"expiry_date\", F.lit(None).cast(DateType())) \\\n",
    "     .withColumn(\"is_current\", F.lit(True))\n",
    "    \n",
    "    # Check if target table exists\n",
    "    if spark.catalog.tableExists(products_target):\n",
    "        # Perform SCD2 merge\n",
    "        target_products = DeltaTable.forName(spark, products_target)\n",
    "        \n",
    "        # Set expiry date for existing records that will be updated\n",
    "        target_products.update(\n",
    "            condition=F.col(\"product_id\").isin([row.product_id for row in df_products_merge.select(\"product_id\").distinct().collect()]),\n",
    "            set={\n",
    "                \"expiry_date\": F.current_date(),\n",
    "                \"is_current\": F.lit(False)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Insert new records\n",
    "        df_products_merge.write.format(\"delta\").mode(\"append\").saveAsTable(products_target)\n",
    "        \n",
    "    else:\n",
    "        # Create new table\n",
    "        df_products_merge.write.format(\"delta\").saveAsTable(products_target)\n",
    "    \n",
    "    print(\"Products merge completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error merging products data: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ad5981-e1fe-43d9-9123-cde61c4e3741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "try:\n",
    "    # Create comprehensive analytics summary\n",
    "    analytics_summary = df_enriched_orders.agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        F.countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "        F.sum(\"order_profit_margin\").alias(\"total_profit\"),\n",
    "        F.avg(\"estimated_clv\").alias(\"avg_estimated_clv\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"profit_margin_percentage\",\n",
    "        F.try_divide(F.col(\"total_profit\"), F.col(\"total_revenue\")) * 100\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"revenue_per_customer\",\n",
    "        F.try_divide(F.col(\"total_revenue\"), F.col(\"unique_customers\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"orders_per_customer\",\n",
    "        F.try_divide(F.col(\"total_orders\"), F.col(\"unique_customers\"))\n",
    "    ) \\\n",
    "    .withColumn(\"report_date\", F.current_date()) \\\n",
    "    .withColumn(\"report_timestamp\", F.current_timestamp())\n",
    "\n",
    "    # Add seasonal analysis\n",
    "    seasonal_analysis = df_enriched_orders.groupBy(\"season\") \\\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"orders_count\"),\n",
    "            F.sum(\"order_amount\").alias(\"seasonal_revenue\"),\n",
    "            F.avg(\"order_amount\").alias(\"avg_seasonal_order_value\")\n",
    "        )\n",
    "\n",
    "    # Add customer segment analysis\n",
    "    segment_analysis = df_customer_analytics.groupBy(\"customer_segment\") \\\n",
    "        .agg(\n",
    "            F.count(\"customer_id\").alias(\"customers_count\"),\n",
    "            F.sum(\"total_spent\").alias(\"segment_revenue\"),\n",
    "            F.avg(\"total_spent\").alias(\"avg_segment_value\")\n",
    "        )\n",
    "\n",
    "    # Add product category analysis\n",
    "    category_analysis = df_product_analytics.groupBy(\"category\") \\\n",
    "        .agg(\n",
    "            F.count(\"product_id\").alias(\"products_count\"),\n",
    "            F.sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_category_revenue\")\n",
    "        )\n",
    "\n",
    "    print(\"✅ Analytics summary created successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating analytics summary: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7466f373-4ea4-43b6-a4e7-8f47cb5f0b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write Analytics Summary to Table\n",
    "try:\n",
    "    # Write main analytics summary\n",
    "    analytics_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(analytics_summary_table)\n",
    "    \n",
    "    # Write seasonal analysis\n",
    "    seasonal_analysis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`incremental_load`.default.seasonal_analysis\")\n",
    "    \n",
    "    # Write segment analysis\n",
    "    segment_analysis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`incremental_load`.default.segment_analysis\")\n",
    "    \n",
    "    # Write category analysis\n",
    "    category_analysis.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"`incremental_load`.default.category_analysis\")\n",
    "    \n",
    "    print(\"Analytics summary tables created successfully\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    final_stats = analytics_summary.collect()[0]\n",
    "\n",
    "    # Helper to safely format None values\n",
    "    def safe_float(value, decimals=2):\n",
    "        if value is None:\n",
    "            return 0.0\n",
    "        return round(float(value), decimals)\n",
    "    \n",
    "    print(\"Final Analytics Summary:\")\n",
    "    print(f\"Total Orders: {final_stats['total_orders']}\")\n",
    "    print(f\"Total Revenue: ${safe_float(final_stats['total_revenue']):,.2f}\")\n",
    "    print(f\"Average Order Value: ${safe_float(final_stats['avg_order_value']):,.2f}\")\n",
    "    print(f\"Unique Customers: {final_stats['unique_customers']}\")\n",
    "    print(f\"Unique Products: {final_stats['unique_products']}\")\n",
    "    print(f\"Total Profit: ${safe_float(final_stats['total_profit']):,.2f}\")\n",
    "    print(f\"Profit Margin: {safe_float(final_stats['profit_margin_percentage']):.2f}%\")\n",
    "    print(f\"Revenue per Customer: ${safe_float(final_stats['revenue_per_customer']):,.2f}\")\n",
    "    print(f\"Orders per Customer: {safe_float(final_stats['orders_per_customer']):.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing analytics summary: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c247616c-9930-4e5a-9ba5-046212934031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Helper to safely convert None to float/int\n",
    "def safe_float(value):\n",
    "    return 0.0 if value is None else float(value)\n",
    "\n",
    "def safe_int(value):\n",
    "    return 0 if value is None else int(value)\n",
    "\n",
    "merge_summary = {\n",
    "    \"archived_files\": None,\n",
    "    \"invalid_records\": None,\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"task\": \"final_merge_operation\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"total_records\": safe_int(final_stats['total_orders']),\n",
    "    \"valid_records\": safe_int(final_stats['total_orders'])\n",
    "}\n",
    "\n",
    "print(\"Final Merge Summary:\")\n",
    "print(json.dumps(merge_summary, indent=2))\n",
    "\n",
    "processing_log_schema = StructType([\n",
    "    StructField(\"archived_files\", LongType(), True),\n",
    "    StructField(\"invalid_records\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"task\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"total_records\", LongType(), True),\n",
    "    StructField(\"valid_records\", LongType(), True)\n",
    "])\n",
    "\n",
    "summary_df = spark.createDataFrame([merge_summary], schema=processing_log_schema)\n",
    "summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"`incremental_load`.default.processing_log\")\n",
    "\n",
    "# Safely set task values\n",
    "dbutils.jobs.taskValues.set(\"final_merge_status\", \"SUCCESS\")\n",
    "dbutils.jobs.taskValues.set(\"total_revenue\", safe_float(final_stats['total_revenue']))\n",
    "dbutils.jobs.taskValues.set(\"total_orders\", safe_int(final_stats['total_orders']))\n",
    "\n",
    "print(\"Event-driven pipeline processing completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_final_merge_operation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
