{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0905c5d-b706-4c89-a169-a595d6826872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Shipping Data Stage Load\n",
    "This notebook processes shipping data from source files and loads it into the staging table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eda000-5163-4e1d-92a9-4b524d3bf5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "source_dir = \"/Volumes/incremental_load/default/incremental_load/shipping_data/source/\"\n",
    "archive_dir = \"/Volumes/incremental_load/default/incremental_load/shipping_data/archive/\"\n",
    "stage_table = \"`incremental_load`.default.shipping_stage\"\n",
    "error_table = \"`incremental_load`.default.shipping_errors\"\n",
    "\n",
    "print(f\"Processing shipping data from: {source_dir}\")\n",
    "print(f\"Staging table: {stage_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433a9801-1295-429f-ac85-e885f32507dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Define schema for shipping data\n",
    "shipping_schema = StructType([\n",
    "    StructField(\"shipping_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"tracking_number\", StringType(), False),\n",
    "    StructField(\"carrier\", StringType(), False),\n",
    "    StructField(\"service_type\", StringType(), False),\n",
    "    StructField(\"origin_warehouse\", StringType(), False),\n",
    "    StructField(\"destination_address\", StringType(), False),\n",
    "    StructField(\"shipping_cost\", DecimalType(10,2), False),\n",
    "    StructField(\"currency\", StringType(), False),\n",
    "    StructField(\"estimated_delivery\", DateType(), False),\n",
    "    StructField(\"actual_delivery\", DateType(), True),\n",
    "    StructField(\"shipping_status\", StringType(), False),\n",
    "    StructField(\"package_weight\", DecimalType(8,2), False),\n",
    "    StructField(\"package_dimensions\", StringType(), False),\n",
    "    StructField(\"insurance_value\", DecimalType(10,2), False),\n",
    "    StructField(\"created_timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "print(\"Schema defined for shipping data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f3e6c5-f33b-43e6-b775-dea31286e0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and validate shipping data\n",
    "try:\n",
    "    # Read CSV files with schema validation\n",
    "    df_shipping = spark.read.schema(shipping_schema).csv(source_dir, header=True, dateFormat=\"yyyy-MM-dd\", timestampFormat=\"yyyy-MM-dd HH:mm:ss\")\n",
    "    \n",
    "    # Add processing metadata\n",
    "    df_shipping = df_shipping.withColumn(\"processed_timestamp\", F.current_timestamp()) \\\n",
    "                            .withColumn(\"batch_id\", F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))) \\\n",
    "                            .withColumn(\"source_system\", F.lit(\"ecommerce_shipping\"))\n",
    "    \n",
    "    # Data quality checks\n",
    "    total_records = df_shipping.count()\n",
    "    null_shipping_ids = df_shipping.filter(F.col(\"shipping_id\").isNull()).count()\n",
    "    invalid_costs = df_shipping.filter(F.col(\"shipping_cost\") < 0).count()\n",
    "    negative_weights = df_shipping.filter(F.col(\"package_weight\") <= 0).count()\n",
    "    invalid_insurance = df_shipping.filter(F.col(\"insurance_value\") < 0).count()\n",
    "    \n",
    "    print(f\"Total records processed: {total_records}\")\n",
    "    print(f\"Records with null shipping_id: {null_shipping_ids}\")\n",
    "    print(f\"Records with invalid costs: {invalid_costs}\")\n",
    "    print(f\"Records with negative weights: {negative_weights}\")\n",
    "    print(f\"Records with invalid insurance: {invalid_insurance}\")\n",
    "    \n",
    "    # Filter out invalid records - Fixed boolean logic\n",
    "    df_valid_shipping = df_shipping.filter(\n",
    "        (F.col(\"shipping_id\").isNotNull()) & \n",
    "        (F.col(\"shipping_cost\") >= 0) & \n",
    "        (F.col(\"package_weight\") > 0) & \n",
    "        (F.col(\"insurance_value\") >= 0)\n",
    "    )\n",
    "    \n",
    "    # Capture invalid records for error handling - Fixed boolean logic\n",
    "    df_invalid_shipping = df_shipping.filter(\n",
    "        (F.col(\"shipping_id\").isNull()) | \n",
    "        (F.col(\"shipping_cost\") < 0) | \n",
    "        (F.col(\"package_weight\") <= 0) | \n",
    "        (F.col(\"insurance_value\") < 0)\n",
    "    )\n",
    "    \n",
    "    valid_records = df_valid_shipping.count()\n",
    "    invalid_records = df_invalid_shipping.count()\n",
    "    \n",
    "    print(f\"Valid records: {valid_records}\")\n",
    "    print(f\"Invalid records: {invalid_records}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading shipping data: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5163b76-307a-4a84-a2bb-15bdbd122a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data enrichment - Shipping analytics\n",
    "try:\n",
    "    # Calculate delivery performance metrics\n",
    "    df_valid_shipping = df_valid_shipping.withColumn(\n",
    "        \"delivery_days\",\n",
    "        F.when(F.col(\"actual_delivery\").isNotNull(),\n",
    "               F.datediff(F.col(\"actual_delivery\"), F.col(\"created_timestamp\").cast(\"date\")))\n",
    "         .otherwise(F.lit(None))\n",
    "    )\n",
    "    \n",
    "    # Calculate estimated delivery days\n",
    "    df_valid_shipping = df_valid_shipping.withColumn(\n",
    "        \"estimated_delivery_days\",\n",
    "        F.datediff(F.col(\"estimated_delivery\"), F.col(\"created_timestamp\").cast(\"date\"))\n",
    "    )\n",
    "    \n",
    "    # Calculate delivery performance\n",
    "    df_valid_shipping = df_valid_shipping.withColumn(\n",
    "        \"delivery_performance\",\n",
    "        F.when(F.col(\"actual_delivery\").isNull(), \"Pending\")\n",
    "         .when(F.col(\"delivery_days\") <= F.col(\"estimated_delivery_days\"), \"On Time\")\n",
    "         .when(F.col(\"delivery_days\") <= F.col(\"estimated_delivery_days\") + 1, \"Slightly Delayed\")\n",
    "         .otherwise(\"Delayed\")\n",
    "    )\n",
    "    \n",
    "    # Create shipping cost categories\n",
    "    df_valid_shipping = df_valid_shipping.withColumn(\n",
    "        \"cost_category\",\n",
    "        F.when(F.col(\"shipping_cost\") < 10, \"Low Cost\")\n",
    "         .when(F.col(\"shipping_cost\") < 20, \"Medium Cost\")\n",
    "         .otherwise(\"High Cost\")\n",
    "    )\n",
    "    \n",
    "    # Calculate cost per weight ratio\n",
    "    df_valid_shipping = df_valid_shipping.withColumn(\n",
    "        \"cost_per_kg\",\n",
    "        F.col(\"shipping_cost\") / F.col(\"package_weight\")\n",
    "    )\n",
    "    \n",
    "    print(\"Data enrichment completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data enrichment: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc17c46-5b42-4ef1-a9ba-bc8ff8a937ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write valid data to staging table\n",
    "try:\n",
    "    # Create or overwrite staging table\n",
    "    df_valid_shipping.write.format(\"delta\").mode(\"overwrite\").saveAsTable(stage_table)\n",
    "    print(f\"Successfully loaded {valid_records} valid shipping records to staging table\")\n",
    "    \n",
    "    # Write invalid records to error table for investigation\n",
    "    if invalid_records > 0:\n",
    "        df_invalid_shipping.withColumn(\"error_reason\", F.lit(\"Data quality validation failed\")) \\\n",
    "                          .withColumn(\"error_timestamp\", F.current_timestamp()) \\\n",
    "                          .write.format(\"delta\").mode(\"append\").saveAsTable(error_table)\n",
    "        print(f\"Logged {invalid_records} invalid records to error table\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing to staging table: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3568f77f-5022-417c-8105-7147a35547bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Archive processed files\n",
    "try:\n",
    "    # List all files in the source directory\n",
    "    files = dbutils.fs.ls(source_dir)\n",
    "    \n",
    "    archived_count = 0\n",
    "    for file in files:\n",
    "        if file.name.endswith('.csv'):\n",
    "            src_path = file.path\n",
    "            archive_path = archive_dir + file.name\n",
    "            \n",
    "            # Move the file to archive\n",
    "            dbutils.fs.mv(src_path, archive_path)\n",
    "            archived_count += 1\n",
    "            print(f\"Archived: {file.name}\")\n",
    "    \n",
    "    print(f\"Successfully archived {archived_count} files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error archiving files: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b468de4c-79d8-4940-af1d-6b7eb9ce1341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log processing summary\n",
    "processing_summary = {\n",
    "    \"task\": \"shipping_stage_load\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"total_records\": total_records,\n",
    "    \"valid_records\": valid_records,\n",
    "    \"invalid_records\": invalid_records,\n",
    "    \"archived_files\": archived_count,\n",
    "    \"status\": \"SUCCESS\" if invalid_records == 0 else \"SUCCESS_WITH_WARNINGS\"\n",
    "}\n",
    "\n",
    "print(\"Processing Summary:\")\n",
    "print(json.dumps(processing_summary, indent=2))\n",
    "\n",
    "# Store summary in a table for monitoring\n",
    "summary_df = spark.createDataFrame([processing_summary])\n",
    "summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"`incremental_load`.default.processing_log\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_shipping_stage_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
