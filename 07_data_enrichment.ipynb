{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb536bbe-23e1-4ed1-ab93-e910acc66c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Enrichment and Business Intelligence\n",
    "This notebook enriches the validated data with additional business metrics and prepares it for analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91886656-be1e-4d54-9085-a45ae7a46c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "orders_stage = \"`incremental_load`.default.orders_stage\"\n",
    "customers_stage = \"`incremental_load`.default.customers_stage\"\n",
    "products_stage = \"`incremental_load`.default.products_stage\"\n",
    "inventory_stage = \"`incremental_load`.default.inventory_stage\"\n",
    "shipping_stage = \"`incremental_load`.default.shipping_stage\"\n",
    "enriched_orders_table = \"`incremental_load`.default.enriched_orders\"\n",
    "customer_analytics_table = \"`incremental_load`.default.customer_analytics\"\n",
    "product_analytics_table = \"`incremental_load`.default.product_analytics\"\n",
    "\n",
    "print(\"Starting data enrichment process...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec85809-fe72-4dd9-a098-bfa8da771ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Read all staging tables\n",
    "try:\n",
    "    df_orders = spark.read.table(orders_stage)\n",
    "    df_customers = spark.read.table(customers_stage)\n",
    "    df_products = spark.read.table(products_stage)\n",
    "    df_inventory = spark.read.table(inventory_stage)\n",
    "    df_shipping = spark.read.table(shipping_stage)\n",
    "    \n",
    "    print(\"Successfully loaded all staging tables for enrichment\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading staging tables: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9a1612-b2d8-4910-9112-b0bcbbe0b291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create enriched orders dataset with all related information\n",
    "try:\n",
    "    # Rename ALL conflicting columns to avoid ambiguity\n",
    "    df_customers_renamed = df_customers.withColumnRenamed(\"created_timestamp\", \"customer_created_timestamp\") \\\n",
    "                                      .withColumnRenamed(\"batch_id\", \"customer_batch_id\") \\\n",
    "                                      .withColumnRenamed(\"processed_timestamp\", \"customer_processed_timestamp\") \\\n",
    "                                      .withColumnRenamed(\"source_system\", \"customer_source_system\") \\\n",
    "                                      .withColumnRenamed(\"lifecycle_stage\", \"customer_lifecycle_stage\")\n",
    "    \n",
    "    df_products_renamed = df_products.withColumnRenamed(\"created_timestamp\", \"product_created_timestamp\") \\\n",
    "                                    .withColumnRenamed(\"batch_id\", \"product_batch_id\") \\\n",
    "                                    .withColumnRenamed(\"processed_timestamp\", \"product_processed_timestamp\") \\\n",
    "                                    .withColumnRenamed(\"source_system\", \"product_source_system\") \\\n",
    "                                    .withColumnRenamed(\"currency\", \"product_currency\") \\\n",
    "                                    .withColumnRenamed(\"weight_kg\", \"product_weight_kg\") \\\n",
    "                                    .withColumnRenamed(\"lifecycle_stage\", \"product_lifecycle_stage\") \\\n",
    "                                    .withColumnRenamed(\"stock_quantity\", \"product_stock_quantity\") \\\n",
    "                                    .withColumnRenamed(\"stock_status\", \"product_stock_status\") \\\n",
    "                                    .withColumnRenamed(\"price_segment\", \"product_price_segment\")\n",
    "    \n",
    "    df_inventory_renamed = df_inventory.withColumnRenamed(\"created_timestamp\", \"inventory_created_timestamp\") \\\n",
    "                                      .withColumnRenamed(\"batch_id\", \"inventory_batch_id\") \\\n",
    "                                      .withColumnRenamed(\"processed_timestamp\", \"inventory_processed_timestamp\") \\\n",
    "                                      .withColumnRenamed(\"source_system\", \"inventory_source_system\") \\\n",
    "                                      .withColumnRenamed(\"stock_status\", \"inventory_stock_status\")\n",
    "    \n",
    "    df_shipping_renamed = df_shipping.withColumnRenamed(\"created_timestamp\", \"shipping_created_timestamp\") \\\n",
    "                                    .withColumnRenamed(\"batch_id\", \"shipping_batch_id\") \\\n",
    "                                    .withColumnRenamed(\"processed_timestamp\", \"shipping_processed_timestamp\") \\\n",
    "                                    .withColumnRenamed(\"source_system\", \"shipping_source_system\") \\\n",
    "                                    .withColumnRenamed(\"currency\", \"shipping_currency\") \\\n",
    "                                    .withColumnRenamed(\"package_weight\", \"shipping_package_weight\")\n",
    "    \n",
    "    # Join orders with customers, products, inventory, and shipping\n",
    "    df_enriched_orders = df_orders \\\n",
    "        .join(df_customers_renamed, \"customer_id\", \"left\") \\\n",
    "        .join(df_products_renamed, \"product_id\", \"left\") \\\n",
    "        .join(df_inventory_renamed, \"product_id\", \"left\") \\\n",
    "        .join(df_shipping_renamed, \"order_id\", \"left\")\n",
    "    \n",
    "    # Add business metrics\n",
    "    df_enriched_orders = df_enriched_orders.withColumn(\n",
    "        \"order_profit_margin\",\n",
    "        F.col(\"order_amount\") * 0.3  # Assuming 30% profit margin\n",
    "    )\n",
    "    \n",
    "    # Add customer lifetime value estimation\n",
    "    df_enriched_orders = df_enriched_orders.withColumn(\n",
    "        \"estimated_clv\",\n",
    "        F.col(\"order_amount\") * F.when(F.col(\"customer_tier\") == \"premium\", 10)\n",
    "                                 .when(F.col(\"customer_tier\") == \"gold\", 7)\n",
    "                                 .when(F.col(\"customer_tier\") == \"silver\", 5)\n",
    "                                 .otherwise(3)\n",
    "    )\n",
    "    \n",
    "    # Add seasonal indicators\n",
    "    df_enriched_orders = df_enriched_orders.withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.month(F.col(\"order_date\")).isin([12, 1, 2]), \"Winter\")\n",
    "         .when(F.month(F.col(\"order_date\")).isin([3, 4, 5]), \"Spring\")\n",
    "         .when(F.month(F.col(\"order_date\")).isin([6, 7, 8]), \"Summer\")\n",
    "         .otherwise(\"Fall\")\n",
    "    )\n",
    "    \n",
    "    # Add day of week and time of day indicators\n",
    "    df_enriched_orders = df_enriched_orders.withColumn(\n",
    "        \"day_of_week\", F.dayofweek(F.col(\"order_date\"))\n",
    "    ).withColumn(\n",
    "        \"is_weekend\", F.when(F.dayofweek(F.col(\"order_date\")).isin([1, 7]), True).otherwise(False)\n",
    "    ).withColumn(\n",
    "        \"time_of_day\",\n",
    "        F.when(F.hour(F.col(\"created_timestamp\")) < 6, \"Early Morning\")\n",
    "         .when(F.hour(F.col(\"created_timestamp\")) < 12, \"Morning\")\n",
    "         .when(F.hour(F.col(\"created_timestamp\")) < 18, \"Afternoon\")\n",
    "         .otherwise(\"Evening\")\n",
    "    )\n",
    "    \n",
    "    print(\"Enriched orders dataset created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating enriched orders: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa4d2f52-c30e-44d3-bf5d-6169cf556c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create customer analytics dataset\n",
    "try:\n",
    "    # Calculate customer metrics\n",
    "    df_customer_analytics = df_enriched_orders.groupBy(\"customer_id\") \\\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            F.sum(\"order_amount\").alias(\"total_spent\"),\n",
    "            F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "            F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\"),\n",
    "            F.countDistinct(\"product_id\").alias(\"unique_products_purchased\"),\n",
    "            F.countDistinct(\"category\").alias(\"unique_categories_purchased\"),\n",
    "            F.sum(\"order_profit_margin\").alias(\"total_profit_generated\"),\n",
    "            F.avg(\"estimated_clv\").alias(\"avg_estimated_clv\")\n",
    "        )\n",
    "    \n",
    "    # Join with customer details\n",
    "    df_customer_analytics = df_customer_analytics.join(df_customers_renamed, \"customer_id\", \"left\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    df_customer_analytics = df_customer_analytics.withColumn(\n",
    "        \"days_since_first_order\",\n",
    "        F.datediff(F.current_date(), F.col(\"first_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"days_since_last_order\",\n",
    "        F.datediff(F.current_date(), F.col(\"last_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"order_frequency_days\",\n",
    "        F.col(\"days_since_first_order\") / F.col(\"total_orders\")\n",
    "    )\n",
    "    \n",
    "    # Create customer segments\n",
    "    df_customer_analytics = df_customer_analytics.withColumn(\n",
    "        \"customer_segment\",\n",
    "        F.when((F.col(\"total_spent\") >= 1000) & (F.col(\"total_orders\") >= 5), \"VIP\")\n",
    "         .when((F.col(\"total_spent\") >= 500) & (F.col(\"total_orders\") >= 3), \"High Value\")\n",
    "         .when((F.col(\"total_spent\") >= 200) & (F.col(\"total_orders\") >= 2), \"Medium Value\")\n",
    "         .otherwise(\"Low Value\")\n",
    "    )\n",
    "    \n",
    "    # Create customer lifecycle stage\n",
    "    df_customer_analytics = df_customer_analytics.withColumn(\n",
    "        \"lifecycle_stage\",\n",
    "        F.when(F.col(\"days_since_last_order\") <= 30, \"Active\")\n",
    "         .when(F.col(\"days_since_last_order\") <= 90, \"At Risk\")\n",
    "         .when(F.col(\"days_since_last_order\") <= 180, \"Inactive\")\n",
    "         .otherwise(\"Lost\")\n",
    "    )\n",
    "    \n",
    "    print(\"Customer analytics dataset created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating customer analytics: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f34bf2f-b0f4-47e1-9c32-442939dd3175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create product analytics dataset\n",
    "try:\n",
    "    # Calculate product metrics\n",
    "    df_product_analytics = df_enriched_orders.groupBy(\"product_id\") \\\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "            F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "            F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "            F.sum(\"order_profit_margin\").alias(\"total_profit\"),\n",
    "            F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\")\n",
    "        )\n",
    "    \n",
    "    # Join with product details\n",
    "    df_product_analytics = df_product_analytics.join(df_products_renamed, \"product_id\", \"left\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    df_product_analytics = df_product_analytics.withColumn(\n",
    "        \"days_since_first_order\",\n",
    "        F.datediff(F.current_date(), F.col(\"first_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"days_since_last_order\",\n",
    "        F.datediff(F.current_date(), F.col(\"last_order_date\"))\n",
    "    ).withColumn(\n",
    "        \"order_frequency_days\",\n",
    "        F.col(\"days_since_first_order\") / F.col(\"total_orders\")\n",
    "    ).withColumn(\n",
    "        \"revenue_per_customer\",\n",
    "        F.col(\"total_revenue\") / F.col(\"unique_customers\")\n",
    "    )\n",
    "    \n",
    "    # Create product performance categories\n",
    "    df_product_analytics = df_product_analytics.withColumn(\n",
    "        \"performance_category\",\n",
    "        F.when((F.col(\"total_revenue\") >= 5000) & (F.col(\"total_orders\") >= 20), \"Star\")\n",
    "         .when((F.col(\"total_revenue\") >= 2000) & (F.col(\"total_orders\") >= 10), \"High Performer\")\n",
    "         .when((F.col(\"total_revenue\") >= 500) & (F.col(\"total_orders\") >= 5), \"Medium Performer\")\n",
    "         .otherwise(\"Low Performer\")\n",
    "    )\n",
    "    \n",
    "    # Create product lifecycle stage\n",
    "    df_product_analytics = df_product_analytics.withColumn(\n",
    "        \"product_lifecycle\",\n",
    "        F.when(F.col(\"days_since_last_order\") <= 30, \"Active\")\n",
    "         .when(F.col(\"days_since_last_order\") <= 90, \"Declining\")\n",
    "         .when(F.col(\"discontinued\") == True, \"Discontinued\")\n",
    "         .otherwise(\"Stagnant\")\n",
    "    )\n",
    "    \n",
    "    print(\"Product analytics dataset created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating product analytics: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd1b005e-5562-46cf-84e1-8159c7fc21ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write enriched datasets to tables\n",
    "try:\n",
    "    # Write enriched orders\n",
    "    df_enriched_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(enriched_orders_table)\n",
    "    print(f\"Successfully wrote enriched orders to {enriched_orders_table}\")\n",
    "    \n",
    "    # Write customer analytics\n",
    "    df_customer_analytics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(customer_analytics_table)\n",
    "    print(f\"Successfully wrote customer analytics to {customer_analytics_table}\")\n",
    "    \n",
    "    # Write product analytics\n",
    "    df_product_analytics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(product_analytics_table)\n",
    "    print(f\"Successfully wrote product analytics to {product_analytics_table}\")\n",
    "    \n",
    "    # Log enrichment statistics\n",
    "    from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "\n",
    "    enrichment_summary = {\n",
    "        \"archived_files\": None,\n",
    "        \"invalid_records\": None,\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"task\": \"data_enrichment\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"total_records\": None,\n",
    "        \"valid_records\": None\n",
    "    }\n",
    "\n",
    "    print(\"Enrichment Summary:\")\n",
    "    print(json.dumps(enrichment_summary, indent=2))\n",
    "\n",
    "    processing_log_schema = StructType([\n",
    "        StructField(\"archived_files\", LongType(), True),\n",
    "        StructField(\"invalid_records\", LongType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"task\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"total_records\", LongType(), True),\n",
    "        StructField(\"valid_records\", LongType(), True)\n",
    "    ])\n",
    "\n",
    "    summary_df = spark.createDataFrame([enrichment_summary], schema=processing_log_schema)\n",
    "    summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"`incremental_load`.default.processing_log\")\n",
    "\n",
    "    dbutils.jobs.taskValues.set(\"enrichment_status\", \"SUCCESS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error writing enriched datasets: {str(e)}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_data_enrichment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
