{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b872e98f-b9f5-483a-86fc-3588e9f99eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Validation and Cross-Reference Checks\n",
    "This notebook performs comprehensive data validation across all staging tables and applies business rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d880e0-f946-45c3-9541-b1c138454f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "orders_stage = \"`incremental_load`.default.orders_stage\"\n",
    "customers_stage = \"`incremental_load`.default.customers_stage\"\n",
    "products_stage = \"`incremental_load`.default.products_stage\"\n",
    "inventory_stage = \"`incremental_load`.default.inventory_stage\"\n",
    "shipping_stage = \"`incremental_load`.default.shipping_stage\"\n",
    "validation_results_table = \"`incremental_load`.default.validation_results\"\n",
    "\n",
    "print(\"Starting comprehensive data validation process...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ccae87-d67e-4f4d-b10c-fbbb5934e9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Read all staging tables\n",
    "try:\n",
    "    df_orders = spark.read.table(orders_stage)\n",
    "    df_customers = spark.read.table(customers_stage)\n",
    "    df_products = spark.read.table(products_stage)\n",
    "    df_inventory = spark.read.table(inventory_stage)\n",
    "    df_shipping = spark.read.table(shipping_stage)\n",
    "    \n",
    "    print(\"Successfully loaded all staging tables\")\n",
    "    print(f\"Orders: {df_orders.count()} records\")\n",
    "    print(f\"Customers: {df_customers.count()} records\")\n",
    "    print(f\"Products: {df_products.count()} records\")\n",
    "    print(f\"Inventory: {df_inventory.count()} records\")\n",
    "    print(f\"Shipping: {df_shipping.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading staging tables: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04311a3f-2a67-40be-9374-4c293ce2fa32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-reference validation: Orders vs Customers\n",
    "try:\n",
    "    # Check for orphaned orders (orders without valid customers)\n",
    "    orphaned_orders = df_orders.join(df_customers, \"customer_id\", \"left_anti\")\n",
    "    orphaned_orders_count = orphaned_orders.count()\n",
    "    \n",
    "    # Check for orphaned customers (customers without any orders)\n",
    "    orphaned_customers = df_customers.join(df_orders, \"customer_id\", \"left_anti\")\n",
    "    orphaned_customers_count = orphaned_customers.count()\n",
    "    \n",
    "    print(f\"Orphaned orders (no valid customer): {orphaned_orders_count}\")\n",
    "    print(f\"Orphaned customers (no orders): {orphaned_customers_count}\")\n",
    "    \n",
    "    # Validate order amounts are reasonable\n",
    "    unreasonable_orders = df_orders.filter(\n",
    "        (F.col(\"order_amount\") < 1) | (F.col(\"order_amount\") > 10000)\n",
    "    )\n",
    "    unreasonable_orders_count = unreasonable_orders.count()\n",
    "    \n",
    "    print(f\"Orders with unreasonable amounts: {unreasonable_orders_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in orders-customers validation: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcbfafd-cb2a-4aac-a056-8dec8b7f4d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-reference validation: Orders vs Products\n",
    "try:\n",
    "    # Check for orphaned orders (orders without valid products)\n",
    "    orphaned_orders_products = df_orders.join(df_products, \"product_id\", \"left_anti\")\n",
    "    orphaned_orders_products_count = orphaned_orders_products.count()\n",
    "    \n",
    "    # Check for orphaned products (products without any orders)\n",
    "    orphaned_products = df_products.join(df_orders, \"product_id\", \"left_anti\")\n",
    "    orphaned_products_count = orphaned_products.count()\n",
    "    \n",
    "    print(f\"Orders with invalid products: {orphaned_orders_products_count}\")\n",
    "    print(f\"Products without orders: {orphaned_products_count}\")\n",
    "    \n",
    "    # Validate order amounts against product prices\n",
    "    orders_with_products = df_orders.join(df_products, \"product_id\", \"inner\")\n",
    "    price_mismatch = orders_with_products.filter(\n",
    "        F.abs(F.col(\"order_amount\") - F.col(\"price\")) > 0.01\n",
    "    )\n",
    "    price_mismatch_count = price_mismatch.count()\n",
    "    \n",
    "    print(f\"Orders with price mismatches: {price_mismatch_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in orders-products validation: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4763ac99-e22c-4505-b3ce-ad4cf206cd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-reference validation: Orders vs Shipping\n",
    "try:\n",
    "    # Check for orders without shipping information\n",
    "    orders_without_shipping = df_orders.join(df_shipping, \"order_id\", \"left_anti\")\n",
    "    orders_without_shipping_count = orders_without_shipping.count()\n",
    "    \n",
    "    # Check for shipping without orders\n",
    "    shipping_without_orders = df_shipping.join(df_orders, \"order_id\", \"left_anti\")\n",
    "    shipping_without_orders_count = shipping_without_orders.count()\n",
    "    \n",
    "    print(f\"Orders without shipping: {orders_without_shipping_count}\")\n",
    "    print(f\"Shipping without orders: {shipping_without_orders_count}\")\n",
    "    \n",
    "    # Validate shipping costs are reasonable\n",
    "    unreasonable_shipping = df_shipping.filter(\n",
    "        (F.col(\"shipping_cost\") < 0) | (F.col(\"shipping_cost\") > 100)\n",
    "    )\n",
    "    unreasonable_shipping_count = unreasonable_shipping.count()\n",
    "    \n",
    "    print(f\"Shipping with unreasonable costs: {unreasonable_shipping_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in orders-shipping validation: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9946a841-fea4-4767-aee1-af3e381ec912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross-reference validation: Products vs Inventory\n",
    "try:\n",
    "    # Check for products without inventory\n",
    "    products_without_inventory = df_products.join(df_inventory, \"product_id\", \"left_anti\")\n",
    "    products_without_inventory_count = products_without_inventory.count()\n",
    "    \n",
    "    # Check for inventory without products\n",
    "    inventory_without_products = df_inventory.join(df_products, \"product_id\", \"left_anti\")\n",
    "    inventory_without_products_count = inventory_without_products.count()\n",
    "    \n",
    "    print(f\"Products without inventory: {products_without_inventory_count}\")\n",
    "    print(f\"Inventory without products: {inventory_without_products_count}\")\n",
    "    \n",
    "    # Validate stock quantities are consistent\n",
    "    products_with_inventory = df_products.join(df_inventory, \"product_id\", \"inner\")\n",
    "    stock_mismatch = products_with_inventory.filter(\n",
    "        F.col(\"products_stage.stock_quantity\") != F.col(\"inventory_stage.stock_quantity\")\n",
    "    )\n",
    "    stock_mismatch_count = stock_mismatch.count()\n",
    "    \n",
    "    print(f\"Stock quantity mismatches: {stock_mismatch_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in products-inventory validation: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca6fb90-554d-46a3-a365-81ff82894425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Business Rules Validation\n",
    "try:\n",
    "    # Rule 1: Premium customers should have higher order values\n",
    "    premium_customers_orders = df_orders.join(df_customers, \"customer_id\", \"inner\") \\\n",
    "                                       .filter(F.col(\"customer_tier\") == \"premium\")\n",
    "    \n",
    "    low_value_premium_orders = premium_customers_orders.filter(F.col(\"order_amount\") < 100)\n",
    "    low_value_premium_count = low_value_premium_orders.count()\n",
    "    \n",
    "    # Rule 2: Orders should be processed within business hours (8 AM - 6 PM)\n",
    "    orders_outside_hours = df_orders.filter(\n",
    "        (F.hour(F.col(\"created_timestamp\")) < 8) | \n",
    "        (F.hour(F.col(\"created_timestamp\")) > 18)\n",
    "    )\n",
    "    orders_outside_hours_count = orders_outside_hours.count()\n",
    "    \n",
    "    # Rule 3: Discontinued products should not have new orders\n",
    "    discontinued_orders = df_orders.join(df_products, \"product_id\", \"inner\") \\\n",
    "                                  .filter(F.col(\"discontinued\") == True)\n",
    "    discontinued_orders_count = discontinued_orders.count()\n",
    "    \n",
    "    print(f\"Premium customers with low-value orders: {low_value_premium_count}\")\n",
    "    print(f\"Orders outside business hours: {orders_outside_hours_count}\")\n",
    "    print(f\"Orders for discontinued products: {discontinued_orders_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in business rules validation: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d367dd-f2d5-48e1-b2ac-1b7c7d078e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compile validation results\n",
    "try:\n",
    "    validation_results = [\n",
    "        {\n",
    "            \"validation_type\": \"orphaned_orders\",\n",
    "            \"count\": orphaned_orders_count,\n",
    "            \"severity\": \"HIGH\" if orphaned_orders_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders without valid customers\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"orphaned_customers\",\n",
    "            \"count\": orphaned_customers_count,\n",
    "            \"severity\": \"MEDIUM\" if orphaned_customers_count > 0 else \"NONE\",\n",
    "            \"description\": \"Customers without any orders\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"unreasonable_orders\",\n",
    "            \"count\": unreasonable_orders_count,\n",
    "            \"severity\": \"HIGH\" if unreasonable_orders_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders with unreasonable amounts\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"orphaned_orders_products\",\n",
    "            \"count\": orphaned_orders_products_count,\n",
    "            \"severity\": \"HIGH\" if orphaned_orders_products_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders with invalid products\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"price_mismatch\",\n",
    "            \"count\": price_mismatch_count,\n",
    "            \"severity\": \"MEDIUM\" if price_mismatch_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders with price mismatches\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"orders_without_shipping\",\n",
    "            \"count\": orders_without_shipping_count,\n",
    "            \"severity\": \"HIGH\" if orders_without_shipping_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders without shipping information\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"unreasonable_shipping\",\n",
    "            \"count\": unreasonable_shipping_count,\n",
    "            \"severity\": \"MEDIUM\" if unreasonable_shipping_count > 0 else \"NONE\",\n",
    "            \"description\": \"Shipping with unreasonable costs\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"products_without_inventory\",\n",
    "            \"count\": products_without_inventory_count,\n",
    "            \"severity\": \"MEDIUM\" if products_without_inventory_count > 0 else \"NONE\",\n",
    "            \"description\": \"Products without inventory\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"low_value_premium_orders\",\n",
    "            \"count\": low_value_premium_count,\n",
    "            \"severity\": \"LOW\" if low_value_premium_count > 0 else \"NONE\",\n",
    "            \"description\": \"Premium customers with low-value orders\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"orders_outside_hours\",\n",
    "            \"count\": orders_outside_hours_count,\n",
    "            \"severity\": \"LOW\" if orders_outside_hours_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders outside business hours\"\n",
    "        },\n",
    "        {\n",
    "            \"validation_type\": \"discontinued_orders\",\n",
    "            \"count\": discontinued_orders_count,\n",
    "            \"severity\": \"HIGH\" if discontinued_orders_count > 0 else \"NONE\",\n",
    "            \"description\": \"Orders for discontinued products\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame from validation results\n",
    "    df_validation_results = spark.createDataFrame(validation_results)\n",
    "    df_validation_results = df_validation_results.withColumn(\"validation_timestamp\", F.current_timestamp()) \\\n",
    "                                                .withColumn(\"batch_id\", F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "    \n",
    "    # Write validation results to table\n",
    "    df_validation_results.write.format(\"delta\").mode(\"append\").saveAsTable(validation_results_table)\n",
    "    \n",
    "    # Calculate overall validation score\n",
    "    high_severity_issues = sum(1 for result in validation_results if result[\"severity\"] == \"HIGH\")\n",
    "    medium_severity_issues = sum(1 for result in validation_results if result[\"severity\"] == \"MEDIUM\")\n",
    "    low_severity_issues = sum(1 for result in validation_results if result[\"severity\"] == \"LOW\")\n",
    "    \n",
    "    overall_status = \"PASS\" if high_severity_issues == 0 else \"FAIL\"\n",
    "    \n",
    "    print(f\"Validation Summary:\")\n",
    "    print(f\"High severity issues: {high_severity_issues}\")\n",
    "    print(f\"Medium severity issues: {medium_severity_issues}\")\n",
    "    print(f\"Low severity issues: {low_severity_issues}\")\n",
    "    print(f\"Overall status: {overall_status}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error compiling validation results: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef5f8f3-751c-4945-a265-b496e660d737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "\n",
    "# Log validation summary\n",
    "validation_summary = {\n",
    "    \"archived_files\": None,\n",
    "    \"invalid_records\": None,\n",
    "    \"status\": None,\n",
    "    \"task\": \"data_validation\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"total_records\": None,\n",
    "    \"valid_records\": None\n",
    "}\n",
    "\n",
    "print(\"Validation Summary:\")\n",
    "print(json.dumps(validation_summary, indent=2))\n",
    "\n",
    "# Explicit schema for processing_log table\n",
    "processing_log_schema = StructType([\n",
    "    StructField(\"archived_files\", LongType(), True),\n",
    "    StructField(\"invalid_records\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"task\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"total_records\", LongType(), True),\n",
    "    StructField(\"valid_records\", LongType(), True)\n",
    "])\n",
    "\n",
    "summary_df = spark.createDataFrame([validation_summary], schema=processing_log_schema)\n",
    "summary_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"`incremental_load`.default.processing_log\")\n",
    "\n",
    "# Set validation status for downstream tasks\n",
    "dbutils.jobs.taskValues.set(\"validation_status\", overall_status)\n",
    "dbutils.jobs.taskValues.set(\"high_severity_count\", high_severity_issues)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_data_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
